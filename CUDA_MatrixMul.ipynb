{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k317huFnG8ae",
        "outputId": "672e1f7b-4f78-4c6b-de29-d57db27fe68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matrix.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "using namespace std;\n",
        "\n",
        "__global__ void matrixMul(float *A, float *B, float *R, int M, int N, int P, int batchOffset) {\n",
        "    int k = threadIdx.x + batchOffset;   // one thread per matrix\n",
        "    if (k >= gridDim.x * blockDim.x) return;\n",
        "\n",
        "    float *a = A + k * M * N;\n",
        "    float *b = B + k * N * P;\n",
        "    float *r = R + k * M * P;\n",
        "\n",
        "    // compute matrix multiplication\n",
        "    for (int i = 0; i < M; i++) {\n",
        "        for (int l = 0; l < P; l++) {\n",
        "            r[i * P + l] = 0.0f;\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                r[i * P + l] += a[i * N + j] * b[j * P + l];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// print one matrix at given index\n",
        "void printMatrixAtIndex(float *A, int index, int M, int N) {\n",
        "    int offset = index * M * N;\n",
        "    for (int i = 0; i < M; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            cout << A[offset + i * N + j] << \" \";\n",
        "        }\n",
        "        cout << endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    if (argc < 6) {\n",
        "        cout << \"Usage: ./matrix <threads> <k> <m> <n> <p>\" << endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int threads = atoi(argv[1]); // threads per block\n",
        "    int K = atoi(argv[2]);\n",
        "    int M = atoi(argv[3]);\n",
        "    int N = atoi(argv[4]);\n",
        "    int P = atoi(argv[5]);\n",
        "\n",
        "    int sizeA = K * M * N;\n",
        "    int sizeB = K * N * P;\n",
        "    int sizeR = K * M * P;\n",
        "\n",
        "    // Host memory\n",
        "    float *h_A = (float*)malloc(sizeA * sizeof(float));\n",
        "    float *h_B = (float*)malloc(sizeB * sizeof(float));\n",
        "    float *h_R = (float*)malloc(sizeR * sizeof(float));\n",
        "\n",
        "    // Initialize random matrices\n",
        "    for (int i = 0; i < sizeA; i++) h_A[i] = rand() % 10;\n",
        "    for (int i = 0; i < sizeB; i++) h_B[i] = rand() % 10;\n",
        "\n",
        "    // Device memory\n",
        "    float *d_A, *d_B, *d_R;\n",
        "    cudaMalloc(&d_A, sizeA * sizeof(float));\n",
        "    cudaMalloc(&d_B, sizeB * sizeof(float));\n",
        "    cudaMalloc(&d_R, sizeR * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_A, h_A, sizeA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, sizeB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_R, 0, sizeR * sizeof(float));\n",
        "\n",
        "    int remaining = K;\n",
        "    int batchOffset = 0;\n",
        "    while (remaining > 0) {\n",
        "        int currentBatchSize = min(remaining, threads);\n",
        "        matrixMul<<<1, currentBatchSize>>>(d_A, d_B, d_R, M, N, P, batchOffset);\n",
        "        cudaDeviceSynchronize();\n",
        "        remaining -= currentBatchSize;\n",
        "        batchOffset += currentBatchSize;\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    // Copy result back\n",
        "    cudaMemcpy(h_R, d_R, sizeR * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Output the 9th(Optional) matrix:\n",
        "    if (K > 9) {\n",
        "        cout << \"Matrix A[9]:\" << endl;\n",
        "        printMatrixAtIndex(h_A, 9, M, N);\n",
        "\n",
        "        cout << \"Matrix B[9]:\" << endl;\n",
        "        printMatrixAtIndex(h_B, 9, N, P);\n",
        "\n",
        "        cout << \"Matrix C[9]:\" << endl;\n",
        "        printMatrixAtIndex(h_R, 9, M, P);\n",
        "    } else {\n",
        "        cout << \"Error: K <= 9, so A[9], B[9], C[9] do not exist.\" << endl;\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_R);\n",
        "    free(h_A); free(h_B); free(h_R);\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "//!nvcc -arch=sm_75 matrix.cu -o matrix\n",
        "//!time ./matrix 400 100 2 2 2 > output.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 matrix.cu -o matrix"
      ],
      "metadata": {
        "id": "kxynDsm_HLpx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./matrix 400 100 2 2 2 > output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQefuQC-HvSG",
        "outputId": "851a7d00-ea9c-49b2-d95a-f758f941da53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "real\t0m0.287s\n",
            "user\t0m0.018s\n",
            "sys\t0m0.214s\n"
          ]
        }
      ]
    }
  ]
}